seed: 42
horizon: 16 
task: 

model:
  target: models.autoencoder.downsample_cvae.DownsampleCVAE
  kwargs:
    model_kwargs:
      action_dim: 5
      hidden_size: 512
      latent_size: 32
      n_encoder_layers: 1
      n_decoder_layers: 1
      n_heads: 1
      horizon: ${horizon}
      dropout: 0.1
      language_feature_dim: 
      low_dim_feature_dim:
      with_obs: True
      with_language: False
      ckpt_path: null

    training_kwargs:
      lr: 1e-4
      warmup_steps: 1000
      loss_kwargs:
        kl_weight: 1e-2

# values taken from one_policy_to_run_them_all/one_policy_to_run_them_all/algorithms/uni_ppo/ppo/default_config.py
joint_attention_encoder: 
  target: models.joint_module.JointAttentionEncoder
  kwargs: 
    model_kwargs: 
      desc_in_size: 3
      desc_out_size: 5
      state_in_size: 2 # currently one value for qpos and qvel for each joint 
      state_out_size: 2 # original implementation: 4 
      softmax_temp: 1.0
      softmax_temp_min: 0.015
      stability_eps: 1e-8

trainer:
  target: lightning.pytorch.trainer.Trainer
  kwargs:
    devices: [0, 1, 2, 3]
    # devices: [0]
    max_epochs: 400
    pretrain_max_epochs: 10
    check_val_every_n_epoch: 1
    log_every_n_steps: 10
    logger:
      target: lightning.pytorch.loggers.wandb.WandbLogger
      kwargs: 
        project: 
        name: 
    num_sanity_val_steps: 2

dataset:
  target: RoLD.datasets.multi_dataset.MultiDataset
  kwargs:
    root_dir: 
    dataset_names:
    data_cfg: 
    horizon: ${horizon}
    get_language: True 
    get_canonical_image: # processed in main.preprocess 
    get_image_dict: # processed in main.preprocess 
    get_low_dim: # processed in main.preprocess 
    average_step_per_episode: # processed in main.preprocess
    feature_type: r3m_resnet34
    # feature_type: clip_ViT-B32

dataloader:
  batch_size: 512
  num_workers: 32
  pin_memory: True
  persistent_workers: True